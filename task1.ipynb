{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c13a68f-7528-4c21-9c97-d86d9808b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest-Path Kernel:\n",
      "Best C (CV): 50 | CV accuracy: 0.8083\n",
      "accuracy: 0.8100\n",
      "precision: 0.7348\n",
      "recall: 0.9700\n",
      "f1: 0.8362\n",
      "roc_auc: 0.8378\n",
      "\n",
      "Weisfeiler–Lehman Kernel:\n",
      "Best C (CV): 50 | CV accuracy: 0.9717\n",
      "accuracy: 0.9700\n",
      "precision: 0.9434\n",
      "recall: 1.0000\n",
      "f1: 0.9709\n",
      "roc_auc: 0.9986\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def generate_graphs(n0=150, n1=150, seed=42,\n",
    "                    n_min=20, n_max=40,\n",
    "                    p_min=0.05, p_max=0.2, m_min=2, m_max=3):\n",
    "    rand = np.random.default_rng(seed)\n",
    "    graphs, labels = [], []\n",
    "\n",
    "    for _ in range(n0):\n",
    "        n = int(rand.integers(n_min, n_max + 1))\n",
    "        p = float(rand.uniform(p_min, p_max))\n",
    "        G = nx.erdos_renyi_graph(n, p)\n",
    "        graphs.append(G)\n",
    "        labels.append(0)\n",
    "\n",
    "    for _ in range(n1):\n",
    "        n = int(rand.integers(n_min, n_max + 1))\n",
    "        m = int(rand.integers(m_min, m_max + 1))\n",
    "        m = max(1, min(m, n - 1))\n",
    "        G = nx.barabasi_albert_graph(n, m)\n",
    "        graphs.append(G)\n",
    "        labels.append(1)\n",
    "\n",
    "    return graphs, np.array(labels, dtype=int)\n",
    "\n",
    "\n",
    "def shortest_path_feature_vector(G, max_len, normalize=True):\n",
    "    spl = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    lengths = []\n",
    "    \n",
    "    for d in spl.values():\n",
    "        lengths.extend([L for L in d.values() if L > 0]) \n",
    "\n",
    "    counts = np.zeros(max_len, dtype=float) \n",
    "    for L in lengths:\n",
    "        if L >= 1 and L <= max_len:\n",
    "            counts[L - 1] += 1.0\n",
    "    if normalize and counts.sum() > 0:\n",
    "        counts /= counts.sum()\n",
    "    return counts\n",
    "\n",
    "\n",
    "def shortest_path_kernel(train_graphs, test_graphs):\n",
    "    all_graphs = list(train_graphs) + list(test_graphs)\n",
    "    # общий максимум длины кратчайших путей чтобы выровнять размерность векторов\n",
    "    max_len = 1\n",
    "    for G in all_graphs:\n",
    "        lmax = 1\n",
    "        for d in nx.all_pairs_shortest_path_length(G):\n",
    "            _, dist_map = d\n",
    "            if dist_map:\n",
    "                lmax = max(lmax, max(dist_map.values()))\n",
    "        max_len = max(max_len, lmax)\n",
    "\n",
    "    Phi_train = np.vstack([shortest_path_feature_vector(G, max_len, normalize=True) for G in train_graphs])\n",
    "    K_train = Phi_train @ Phi_train.T\n",
    "\n",
    "    Phi_test = np.vstack([shortest_path_feature_vector(G, max_len, normalize=True) for G in test_graphs])\n",
    "    K_test = Phi_test @ Phi_train.T\n",
    "\n",
    "    return K_train, K_test\n",
    "\n",
    "\n",
    "def wl_features(graphs, h=3, use_degree_init=True, normalize=True):\n",
    "    labels_per_graph = []\n",
    "    for G in graphs:\n",
    "        if use_degree_init:\n",
    "            labels = {v: f\"d{G.degree(v)}\" for v in G.nodes()}\n",
    "        else:\n",
    "            labels = {v: \"c0\" for v in G.nodes()}\n",
    "        labels_per_graph.append(labels)\n",
    "\n",
    "    vocab = {}\n",
    "    def ensure_id(token):\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)\n",
    "        return vocab[token]\n",
    "\n",
    "    feat_counts = [Counter() for _ in graphs]\n",
    "\n",
    "    #0\n",
    "    for gi, labels in enumerate(labels_per_graph):\n",
    "        for lbl in labels.values():\n",
    "            idx = ensure_id(f\"0|{lbl}\")\n",
    "            feat_counts[gi][idx] += 1\n",
    "\n",
    "    # wl итерации\n",
    "    current = labels_per_graph\n",
    "    for it in range(1, h + 1):\n",
    "        new_all = []\n",
    "        for gi, G in enumerate(graphs):\n",
    "            labels = current[gi]\n",
    "            new_labels = {}\n",
    "            for v in G.nodes():\n",
    "                multiset = sorted(labels[nb] for nb in G.neighbors(v))\n",
    "                new_lbl = f\"{labels[v]}|{'_'.join(multiset)}\"\n",
    "                token = f\"{it}|{new_lbl}\" \n",
    "                new_labels[v] = token\n",
    "                idx = ensure_id(token)\n",
    "                feat_counts[gi][idx] += 1\n",
    "            new_all.append(new_labels)\n",
    "        current = new_all\n",
    "        \n",
    "    dim = len(vocab)\n",
    "    X = np.zeros((len(graphs), dim), dtype=float)\n",
    "    for i, cnt in enumerate(feat_counts):\n",
    "        for j, val in cnt.items():\n",
    "            X[i, j] = val\n",
    "        if normalize and X[i].sum() > 0:\n",
    "            X[i] /= X[i].sum()\n",
    "    return X\n",
    "\n",
    "\n",
    "def wl_kernel(train_graphs, test_graphs, h=3):\n",
    "    all_graphs = list(train_graphs) + list(test_graphs)\n",
    "    X_all = wl_features(all_graphs, h=h, use_degree_init=True, normalize=True)\n",
    "    n_tr = len(train_graphs)\n",
    "    X_tr, X_te = X_all[:n_tr], X_all[n_tr:]\n",
    "    K_train = X_tr @ X_tr.T\n",
    "    K_test = X_te @ X_tr.T\n",
    "    return K_train, K_test\n",
    "\n",
    "\n",
    "\n",
    "def tune_and_fit_svc(K_train, y, C_grid=(0.1, 1, 10, 25, 50), cv_splits=5, seed=0):\n",
    "    best_C, best_score = None, -np.inf\n",
    "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=seed)\n",
    "    for C in C_grid:\n",
    "        scores = []\n",
    "        for tr_idx, va_idx in skf.split(np.zeros(len(y)), y):\n",
    "            K_tr = K_train[np.ix_(tr_idx, tr_idx)]\n",
    "            K_va = K_train[np.ix_(va_idx, tr_idx)]\n",
    "            clf = SVC(kernel='precomputed', C=C)\n",
    "            clf.fit(K_tr, y[tr_idx])\n",
    "            y_hat = clf.predict(K_va)\n",
    "            scores.append(accuracy_score(y[va_idx], y_hat))\n",
    "        mean_acc = float(np.mean(scores))\n",
    "        if mean_acc > best_score:\n",
    "            best_score, best_C = mean_acc, C\n",
    "\n",
    "    clf = SVC(kernel='precomputed', C=best_C)\n",
    "    clf.fit(K_train, y)\n",
    "    return clf, best_C, best_score\n",
    "\n",
    "\n",
    "def evaluate(clf, K_test, y_test):\n",
    "    y_pred = clf.predict(K_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', zero_division=0)\n",
    "    scores = clf.decision_function(K_test)\n",
    "    auc = roc_auc_score(y_test, scores)\n",
    "    return dict(accuracy=acc, precision=prec, recall=rec, f1=f1, roc_auc=auc)\n",
    "\n",
    "# 1)data\n",
    "graphs, y = generate_graphs(n0=400, n1=400, seed=0)\n",
    "G_tr, G_te, y_tr, y_te = train_test_split(graphs, y, test_size=0.25, random_state=0, stratify=y)\n",
    "\n",
    "# 2)shortest path kernel\n",
    "Ktr_sp, Kte_sp = shortest_path_kernel(G_tr, G_te)\n",
    "sp_clf, sp_C, sp_cv = tune_and_fit_svc(Ktr_sp, y_tr, C_grid=(0.1, 1, 10, 25, 50), cv_splits=5, seed=0)\n",
    "sp_metrics = evaluate(sp_clf, Kte_sp, y_te)\n",
    "\n",
    "print(\"Shortest-Path Kernel:\")\n",
    "print(f\"Best C (CV): {sp_C} | CV accuracy: {sp_cv:.4f}\")\n",
    "for k, v in sp_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# 3)wl kernel\n",
    "Ktr_wl, Kte_wl = wl_kernel(G_tr, G_te, h=3)\n",
    "wl_clf, wl_C, wl_cv = tune_and_fit_svc(Ktr_wl, y_tr, C_grid=(0.1, 1, 10, 25, 50), cv_splits=5, seed=0)\n",
    "wl_metrics = evaluate(wl_clf, Kte_wl, y_te)\n",
    "\n",
    "print(\"\\nWeisfeiler–Lehman Kernel:\")\n",
    "print(f\"Best C (CV): {wl_C} | CV accuracy: {wl_cv:.4f}\")\n",
    "for k, v in wl_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be5eaa6c-23d3-4238-a96e-ea4845b75867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Mean ± Std over 10 runs ===\n",
      "\n",
      "Shortest-Path Kernel:\n",
      "accuracy  : 0.8025 ± 0.0285\n",
      "precision : 0.7330 ± 0.0320\n",
      "recall    : 0.9560 ± 0.0314\n",
      "f1        : 0.8291 ± 0.0223\n",
      "roc_auc   : 0.8603 ± 0.0271\n",
      "\n",
      "Weisfeiler–Lehman Kernel:\n",
      "accuracy  : 0.9700 ± 0.0110\n",
      "precision : 0.9573 ± 0.0134\n",
      "recall    : 0.9840 ± 0.0111\n",
      "f1        : 0.9704 ± 0.0107\n",
      "roc_auc   : 0.9954 ± 0.0044\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 10\n",
    "sp_results, wl_results = [], []\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    graphs, y = generate_graphs(n0=400, n1=400, seed=run)\n",
    "    G_tr, G_te, y_tr, y_te = train_test_split(\n",
    "        graphs, y, test_size=0.25, random_state=run, stratify=y\n",
    "    )\n",
    "\n",
    "    # Shortest-Path Kernel\n",
    "    Ktr_sp, Kte_sp = shortest_path_kernel(G_tr, G_te)\n",
    "    sp_clf, sp_C, sp_cv = tune_and_fit_svc(\n",
    "        Ktr_sp, y_tr, C_grid=(0.1, 1, 10, 25, 50), cv_splits=5, seed=run\n",
    "    )\n",
    "    sp_metrics = evaluate(sp_clf, Kte_sp, y_te)\n",
    "    sp_results.append(sp_metrics)\n",
    "\n",
    "    # Weisfeiler–Lehman Kernel\n",
    "    Ktr_wl, Kte_wl = wl_kernel(G_tr, G_te, h=3)\n",
    "    wl_clf, wl_C, wl_cv = tune_and_fit_svc(\n",
    "        Ktr_wl, y_tr, C_grid=(0.1, 1, 10, 25, 50), cv_splits=5, seed=run\n",
    "    )\n",
    "    wl_metrics = evaluate(wl_clf, Kte_wl, y_te)\n",
    "    wl_results.append(wl_metrics)\n",
    "\n",
    "# усредняем\n",
    "def mean_std(results, key):\n",
    "    vals = [r[key] for r in results]\n",
    "    return np.mean(vals), np.std(vals)\n",
    "\n",
    "print(\"=== Mean ± Std over 10 runs ===\")\n",
    "for name, results in [(\"Shortest-Path\", sp_results), (\"Weisfeiler–Lehman\", wl_results)]:\n",
    "    print(f\"\\n{name} Kernel:\")\n",
    "    for k in results[0].keys():\n",
    "        mean, std = mean_std(results, k)\n",
    "        print(f\"{k:10s}: {mean:.4f} ± {std:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
