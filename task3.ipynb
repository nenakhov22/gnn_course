{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict, Any, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import itertools"
      ],
      "metadata": {
        "id": "fAMj2DXRPWVZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "np6xnv1VPQXC"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def f1_macro(logits: torch.Tensor, y: torch.Tensor, mask: torch.Tensor) -> float:\n",
        "    preds = logits.argmax(dim=-1)[mask].detach().cpu().numpy()\n",
        "    true = y[mask].detach().cpu().numpy()\n",
        "    if len(true) == 0:\n",
        "        return 0.0\n",
        "    return f1_score(true, preds, average=\"macro\")\n",
        "\n",
        "\n",
        "\n",
        "def load_cora(device: torch.device):\n",
        "    #Cora — классический датасет для классификации вершин.\n",
        "    #В Planetoid уже есть train/val/test маски.\n",
        "\n",
        "    dataset = Planetoid(\n",
        "        root=\"data/Planetoid\",\n",
        "        name=\"Cora\",\n",
        "        transform=NormalizeFeatures()\n",
        "    )\n",
        "    data = dataset[0].to(device)\n",
        "    return dataset, data\n",
        "\n",
        "\n",
        "\n",
        "class GCN_PyG(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels: int, hidden_channels: int,\n",
        "                 out_channels: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class MyGCNConv(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, bias: bool = True):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.weight = nn.Parameter(torch.Tensor(in_channels, out_channels))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_normalized_adj(edge_index: torch.Tensor, num_nodes: int,\n",
        "                              device: torch.device) -> torch.sparse.FloatTensor:\n",
        "\n",
        "        loop_index = torch.arange(num_nodes, device=device)\n",
        "        loop_index = loop_index.unsqueeze(0).repeat(2, 1)\n",
        "        edge_index_full = torch.cat([edge_index, loop_index], dim=1)\n",
        "\n",
        "        row, col = edge_index_full\n",
        "\n",
        "        values = torch.ones(row.size(0), device=device)\n",
        "\n",
        "        deg = torch.zeros(num_nodes, device=device)\n",
        "        deg.scatter_add_(0, row, values)\n",
        "\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0.0\n",
        "\n",
        "        norm_values = deg_inv_sqrt[row] * values * deg_inv_sqrt[col]\n",
        "\n",
        "        adj = torch.sparse_coo_tensor(\n",
        "            indices=edge_index_full,\n",
        "            values=norm_values,\n",
        "            size=(num_nodes, num_nodes),\n",
        "            device=device\n",
        "        )\n",
        "        adj = adj.coalesce()\n",
        "        return adj\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        num_nodes = x.size(0)\n",
        "        device = x.device\n",
        "\n",
        "        adj_norm = self._build_normalized_adj(edge_index, num_nodes, device)\n",
        "\n",
        "        support = torch.sparse.mm(adj_norm, x)\n",
        "\n",
        "        out = support @ self.weight\n",
        "\n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class GCN_Custom(nn.Module):\n",
        "    def __init__(self, in_channels: int, hidden_channels: int,\n",
        "                 out_channels: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.conv1 = MyGCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = MyGCNConv(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    hidden_channels: int = 16\n",
        "    dropout: float = 0.5\n",
        "    lr: float = 0.01\n",
        "    weight_decay: float = 5e-4\n",
        "    epochs: int = 200\n",
        "\n",
        "def train_one_epoch(model: nn.Module,\n",
        "                    data,\n",
        "                    optimizer: Adam) -> float:\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model: nn.Module, data) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "\n",
        "    train_f1 = f1_macro(out, data.y, data.train_mask)\n",
        "    val_f1   = f1_macro(out, data.y, data.val_mask)\n",
        "    test_f1  = f1_macro(out, data.y, data.test_mask)\n",
        "\n",
        "    return {\n",
        "        \"train_f1\": train_f1,\n",
        "        \"val_f1\": val_f1,\n",
        "        \"test_f1\": test_f1\n",
        "    }\n",
        "\n",
        "def run_experiments(ModelClass,\n",
        "                    data,\n",
        "                    device: torch.device,\n",
        "                    config_grid: List[TrainConfig],\n",
        "                    label: str):\n",
        "    print(f\"\\nExperiments for {label}\")\n",
        "\n",
        "    best_val_acc = -1.0\n",
        "    best_metrics = None\n",
        "    best_config = None\n",
        "    best_state_dict = None\n",
        "\n",
        "    for i, cfg in enumerate(config_grid, start=1):\n",
        "        print(f\"\\nConfig {i}/{len(config_grid)}: {cfg}\")\n",
        "        model = ModelClass(\n",
        "            in_channels=data.num_features,\n",
        "            hidden_channels=cfg.hidden_channels,\n",
        "            out_channels=int(data.y.max().item()) + 1,\n",
        "            dropout=cfg.dropout\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = Adam(\n",
        "            model.parameters(),\n",
        "            lr=cfg.lr,\n",
        "            weight_decay=cfg.weight_decay\n",
        "        )\n",
        "\n",
        "        for epoch in range(1, cfg.epochs + 1):\n",
        "            loss = train_one_epoch(model, data, optimizer)\n",
        "\n",
        "        # Оценка после обучения\n",
        "        final_metrics = evaluate_model(model, data)\n",
        "        print(\n",
        "            f\"Final metrics (Config {i}): \"\n",
        "            f\"train_f1={final_metrics['train_f1']*100:.2f}%, \"\n",
        "            f\"val_f1={final_metrics['val_f1']*100:.2f}%, \"\n",
        "            f\"test_f1={final_metrics['test_f1']*100:.2f}%\"\n",
        "        )\n",
        "\n",
        "        if final_metrics[\"val_f1\"] > best_val_acc:\n",
        "          best_val_acc = final_metrics[\"val_f1\"]\n",
        "          best_metrics = final_metrics\n",
        "          best_config = cfg\n",
        "          best_state_dict = model.state_dict()\n",
        "\n",
        "    print(f\"\\nBest config for {label}\")\n",
        "    print(best_config)\n",
        "    print(\n",
        "        f\"Best metrics: \"\n",
        "        f\"train_f1={best_metrics['train_f1']*100:.2f}%, \"\n",
        "        f\"val_f1={best_metrics['val_f1']*100:.2f}%, \"\n",
        "        f\"test_f1={best_metrics['test_f1']*100:.2f}%\"\n",
        "    )\n",
        "\n",
        "    return best_config, best_metrics, best_state_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "device = get_device()\n",
        "print(\"Device:\", device)\n",
        "\n",
        "dataset, data = load_cora(device)\n",
        "print(f\"Dataset: {dataset}\")\n",
        "print(f\"Num nodes: {data.num_nodes}\")\n",
        "print(f\"Num features: {data.num_features}\")\n",
        "print(f\"Num classes: {int(data.y.max().item()) + 1}\")\n",
        "\n",
        "hidden_choices  = [16, 32, 64]\n",
        "dropout_choices = [0.5]\n",
        "lr_choices      = [0.01, 0.005, 0.001]\n",
        "wd_choices      = [5e-4, 5e-3]\n",
        "epochs_choices  = [200]\n",
        "\n",
        "config_grid = [\n",
        "    TrainConfig(\n",
        "        hidden_channels=h,\n",
        "        dropout=d,\n",
        "        lr=lr,\n",
        "        weight_decay=wd,\n",
        "        epochs=ep\n",
        "    )\n",
        "    for h, d, lr, wd, ep in itertools.product(\n",
        "        hidden_choices,\n",
        "        dropout_choices,\n",
        "        lr_choices,\n",
        "        wd_choices,\n",
        "        epochs_choices,\n",
        "    )\n",
        "]\n",
        "\n",
        "pyg_best_cfg, pyg_best_metrics, pyg_state = run_experiments(\n",
        "    ModelClass=GCN_PyG,\n",
        "    data=data,\n",
        "    device=device,\n",
        "    config_grid=config_grid,\n",
        "    label=\"GCN (PyG GCNConv)\"\n",
        ")\n",
        "\n",
        "custom_best_cfg, custom_best_metrics, custom_state = run_experiments(\n",
        "    ModelClass=GCN_Custom,\n",
        "    data=data,\n",
        "    device=device,\n",
        "    config_grid=config_grid,\n",
        "    label=\"GCN (Custom MyGCNConv)\"\n",
        ")\n",
        "\n",
        "print(\"PyG GCNConv best:\")\n",
        "print(f\"  config: {pyg_best_cfg}\")\n",
        "print(\n",
        "    f\"  metrics: train_f1={pyg_best_metrics['train_f1']*100:.2f}%, \"\n",
        "    f\"val_f1={pyg_best_metrics['val_f1']*100:.2f}%, \"\n",
        "    f\"test_f1={pyg_best_metrics['test_f1']*100:.2f}%\"\n",
        ")\n",
        "\n",
        "print(\"\\nCustom MyGCNConv best:\")\n",
        "print(f\"  config: {custom_best_cfg}\")\n",
        "print(\n",
        "    f\"  metrics: train_f1={custom_best_metrics['train_f1']*100:.2f}%, \"\n",
        "    f\"val_f1={custom_best_metrics['val_f1']*100:.2f}%, \"\n",
        "    f\"test_f1={custom_best_metrics['test_f1']*100:.2f}%\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AodAXnCoQo5O",
        "outputId": "ed0a1d71-0084-4007-fc92-1c9dd5c0c682"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Dataset: Cora()\n",
            "Num nodes: 2708\n",
            "Num features: 1433\n",
            "Num classes: 7\n",
            "\n",
            "Experiments for GCN (PyG GCNConv)\n",
            "\n",
            "Config 1/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.01, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 1): train_f1=100.00%, val_f1=76.84%, test_f1=78.78%\n",
            "\n",
            "Config 2/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.01, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 2): train_f1=77.95%, val_f1=66.70%, test_f1=63.84%\n",
            "\n",
            "Config 3/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.005, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 3): train_f1=99.29%, val_f1=77.79%, test_f1=80.27%\n",
            "\n",
            "Config 4/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.005, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 4): train_f1=75.78%, val_f1=56.90%, test_f1=56.35%\n",
            "\n",
            "Config 5/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.001, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 5): train_f1=95.69%, val_f1=74.73%, test_f1=76.88%\n",
            "\n",
            "Config 6/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.001, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 6): train_f1=71.86%, val_f1=52.16%, test_f1=51.50%\n",
            "\n",
            "Config 7/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.01, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 7): train_f1=100.00%, val_f1=78.87%, test_f1=80.22%\n",
            "\n",
            "Config 8/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.01, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 8): train_f1=76.78%, val_f1=63.29%, test_f1=63.37%\n",
            "\n",
            "Config 9/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.005, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 9): train_f1=99.29%, val_f1=78.16%, test_f1=80.77%\n",
            "\n",
            "Config 10/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.005, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 10): train_f1=76.77%, val_f1=59.43%, test_f1=61.71%\n",
            "\n",
            "Config 11/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.001, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 11): train_f1=95.71%, val_f1=73.53%, test_f1=77.36%\n",
            "\n",
            "Config 12/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.001, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 12): train_f1=67.54%, val_f1=53.34%, test_f1=55.67%\n",
            "\n",
            "Config 13/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.01, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 13): train_f1=100.00%, val_f1=78.50%, test_f1=79.86%\n",
            "\n",
            "Config 14/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.01, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 14): train_f1=79.20%, val_f1=66.49%, test_f1=62.88%\n",
            "\n",
            "Config 15/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.005, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 15): train_f1=100.00%, val_f1=78.25%, test_f1=80.34%\n",
            "\n",
            "Config 16/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.005, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 16): train_f1=77.77%, val_f1=61.34%, test_f1=62.65%\n",
            "\n",
            "Config 17/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.001, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 17): train_f1=98.59%, val_f1=77.98%, test_f1=80.17%\n",
            "\n",
            "Config 18/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.001, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 18): train_f1=76.02%, val_f1=61.61%, test_f1=63.52%\n",
            "\n",
            "Best config for GCN (PyG GCNConv)\n",
            "TrainConfig(hidden_channels=32, dropout=0.5, lr=0.01, weight_decay=0.0005, epochs=200)\n",
            "Best metrics: train_f1=100.00%, val_f1=78.87%, test_f1=80.22%\n",
            "\n",
            "Experiments for GCN (Custom MyGCNConv)\n",
            "\n",
            "Config 1/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.01, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 1): train_f1=99.29%, val_f1=77.30%, test_f1=78.48%\n",
            "\n",
            "Config 2/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.01, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 2): train_f1=77.71%, val_f1=61.21%, test_f1=61.85%\n",
            "\n",
            "Config 3/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.005, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 3): train_f1=98.55%, val_f1=78.05%, test_f1=81.22%\n",
            "\n",
            "Config 4/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.005, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 4): train_f1=71.07%, val_f1=53.86%, test_f1=54.69%\n",
            "\n",
            "Config 5/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.001, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 5): train_f1=96.46%, val_f1=74.15%, test_f1=76.75%\n",
            "\n",
            "Config 6/18: TrainConfig(hidden_channels=16, dropout=0.5, lr=0.001, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 6): train_f1=77.87%, val_f1=62.17%, test_f1=61.76%\n",
            "\n",
            "Config 7/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.01, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 7): train_f1=100.00%, val_f1=78.13%, test_f1=80.04%\n",
            "\n",
            "Config 8/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.01, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 8): train_f1=78.60%, val_f1=64.85%, test_f1=65.48%\n",
            "\n",
            "Config 9/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.005, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 9): train_f1=100.00%, val_f1=78.63%, test_f1=80.65%\n",
            "\n",
            "Config 10/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.005, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 10): train_f1=76.81%, val_f1=62.23%, test_f1=61.20%\n",
            "\n",
            "Config 11/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.001, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 11): train_f1=98.59%, val_f1=76.14%, test_f1=76.89%\n",
            "\n",
            "Config 12/18: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.001, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 12): train_f1=77.32%, val_f1=55.15%, test_f1=58.65%\n",
            "\n",
            "Config 13/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.01, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 13): train_f1=100.00%, val_f1=78.25%, test_f1=79.97%\n",
            "\n",
            "Config 14/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.01, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 14): train_f1=80.80%, val_f1=67.85%, test_f1=65.59%\n",
            "\n",
            "Config 15/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.005, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 15): train_f1=100.00%, val_f1=78.81%, test_f1=80.29%\n",
            "\n",
            "Config 16/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.005, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 16): train_f1=78.93%, val_f1=63.00%, test_f1=61.48%\n",
            "\n",
            "Config 17/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.001, weight_decay=0.0005, epochs=200)\n",
            "Final metrics (Config 17): train_f1=97.85%, val_f1=78.82%, test_f1=80.17%\n",
            "\n",
            "Config 18/18: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.001, weight_decay=0.005, epochs=200)\n",
            "Final metrics (Config 18): train_f1=73.69%, val_f1=62.69%, test_f1=61.67%\n",
            "\n",
            "Best config for GCN (Custom MyGCNConv)\n",
            "TrainConfig(hidden_channels=64, dropout=0.5, lr=0.001, weight_decay=0.0005, epochs=200)\n",
            "Best metrics: train_f1=97.85%, val_f1=78.82%, test_f1=80.17%\n",
            "PyG GCNConv best:\n",
            "  config: TrainConfig(hidden_channels=32, dropout=0.5, lr=0.01, weight_decay=0.0005, epochs=200)\n",
            "  metrics: train_f1=100.00%, val_f1=78.87%, test_f1=80.22%\n",
            "\n",
            "Custom MyGCNConv best:\n",
            "  config: TrainConfig(hidden_channels=64, dropout=0.5, lr=0.001, weight_decay=0.0005, epochs=200)\n",
            "  metrics: train_f1=97.85%, val_f1=78.82%, test_f1=80.17%\n"
          ]
        }
      ]
    }
  ]
}